{"cells":[{"cell_type":"markdown","metadata":{"id":"q7V9CkxqcdcA"},"source":["# AutoregressiveModel\n","\n","**Description:** AutoregressiveModel implemented in Keras to generate image.\n","\n","**Objective:** The objective of this assignment is to practise using the TensorFlow machine learning framework\n","through implementing custom training modules and data reader modules for image generation on\n","the Chinese Calligraphy dataset using a convolutional neural network (CNN) based architecture.\n","Throughout the assignment, students will be guided to develop the CNN-based model step by\n","step and study how to build custom modules on TensorFlow and the effects of different model\n","configurations."]},{"cell_type":"markdown","metadata":{"id":"N5qADsjVcdcG"},"source":["## Introduction\n","\n","Image generation is one of the fundamental computer vision tasks, referring to the process of generating new images that are visually realistic and similar to real-world images. It is widely used in many applications, such as super resolution, photograph editing and 3D modelling. \n","\n","One approach to image generation is to use models that learn to predict the probability distribution of pixel values, given the values of all the previous pixels. These models generate images one pixel at a time, using the previously generated pixels to condition the generation of the next pixel."]},{"cell_type":"markdown","metadata":{"id":"E6pmZLclieHE"},"source":["### Setting environment\n","\n","Note: You can only use the packages listed below !!!"]},{"cell_type":"code","execution_count":26,"metadata":{"executionInfo":{"elapsed":3883,"status":"ok","timestamp":1680352293862,"user":{"displayName":"Wing Piu HS","userId":"04946939139592579559"},"user_tz":-480},"id":"nzuClBlOcdcI"},"outputs":[],"source":["import numpy as np\n","import math\n","import os\n","from PIL import Image\n","import time\n","from tqdm import tqdm\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1442,"status":"ok","timestamp":1680352295292,"user":{"displayName":"Wing Piu HS","userId":"04946939139592579559"},"user_tz":-480},"id":"UreaZiQ1JiEl","outputId":"f26f08f5-2bc4-43f5-fdba-5fb13713afac"},"outputs":[{"name":"stdout","output_type":"stream","text":["2.12.0\n","1:  [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n","2:  <function is_built_with_cuda at 0x7f2d832bf820>\n","3:  /device:GPU:0\n","4:  [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"]},{"name":"stderr","output_type":"stream","text":["2023-04-03 16:39:51.238090: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n","Your kernel may have been built without NUMA support.\n","2023-04-03 16:39:51.254029: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n","Your kernel may have been built without NUMA support.\n","2023-04-03 16:39:51.254074: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n","Your kernel may have been built without NUMA support.\n","2023-04-03 16:39:51.256106: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n","Your kernel may have been built without NUMA support.\n","2023-04-03 16:39:51.256163: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n","Your kernel may have been built without NUMA support.\n","2023-04-03 16:39:51.256188: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n","Your kernel may have been built without NUMA support.\n","2023-04-03 16:39:51.985936: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n","Your kernel may have been built without NUMA support.\n","2023-04-03 16:39:51.986011: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n","Your kernel may have been built without NUMA support.\n","2023-04-03 16:39:51.986020: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1722] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n","2023-04-03 16:39:51.986048: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n","Your kernel may have been built without NUMA support.\n","2023-04-03 16:39:51.986072: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /device:GPU:0 with 5863 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080, pci bus id: 0000:01:00.0, compute capability: 7.5\n"]}],"source":["from tensorflow.python.ops.numpy_ops import np_config\n","np_config.enable_numpy_behavior()\n","\n","print(tf.__version__)\n","\n","print('1: ', tf.config.list_physical_devices('GPU'))\n","print('2: ', tf.test.is_built_with_cuda)\n","print('3: ', tf.test.gpu_device_name())\n","print('4: ', tf.config.get_visible_devices())\n"]},{"cell_type":"markdown","metadata":{"id":"fO2uXmIzcdcK"},"source":["## Getting the data\n"]},{"cell_type":"markdown","metadata":{"id":"g7-ajC_RQxam"},"source":["### Download dataset\n","\n"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1680352295292,"user":{"displayName":"Wing Piu HS","userId":"04946939139592579559"},"user_tz":-480},"id":"cPs8yqGRNWix"},"outputs":[],"source":["# Download dataset from google drive\n","# ! wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=18ogOIVtYFkcCyNN6AHLCrTI95zMrYAZt' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=18ogOIVtYFkcCyNN6AHLCrTI95zMrYAZt\" -O calligraphy.zip && rm -rf /tmp/cookies.txt\n","# ! mkdir ./data && unzip -q calligraphy.zip -d ./data/ && rm calligraphy.zip\n","# ! wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1w7JVXz6U-NVDZxBf1oSAVjKdR4BJs1zI' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1w7JVXz6U-NVDZxBf1oSAVjKdR4BJs1zI\" -O calligraphy.zip && rm -rf /tmp/cookies.txt\n","# ! unzip -q calligraphy.zip -d ./data/ && rm calligraphy.zip\n","# ! ls -l ./data"]},{"cell_type":"markdown","metadata":{"id":"RrivlNhIRIgp"},"source":["### make dataset"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1680352295292,"user":{"displayName":"Wing Piu HS","userId":"04946939139592579559"},"user_tz":-480},"id":"EWlGYkLdRGYo"},"outputs":[],"source":["# Model / data parameters\n","input_shape = (32, 32, 1)\n","batch_size = 32\n","data_dir = \"./train\"\n","data_name = \"calligraphy\""]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":45299,"status":"ok","timestamp":1680352340589,"user":{"displayName":"Wing Piu HS","userId":"04946939139592579559"},"user_tz":-480},"id":"adlqUIs5VXZd","outputId":"47c5ef9c-9462-48eb-ac42-f24e3e141313"},"outputs":[{"name":"stderr","output_type":"stream","text":["2023-04-03 16:39:57.237950: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n","Your kernel may have been built without NUMA support.\n","2023-04-03 16:39:57.238043: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n","Your kernel may have been built without NUMA support.\n","2023-04-03 16:39:57.238074: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n","Your kernel may have been built without NUMA support.\n","2023-04-03 16:39:57.238352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n","Your kernel may have been built without NUMA support.\n","2023-04-03 16:39:57.238365: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1722] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n","2023-04-03 16:39:57.238394: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n","Your kernel may have been built without NUMA support.\n","2023-04-03 16:39:57.238408: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5863 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080, pci bus id: 0000:01:00.0, compute capability: 7.5\n"]},{"name":"stdout","output_type":"stream","text":["1313 (32, 32, 32, 1) (32, 32, 32, 1)\n"]}],"source":["# dataset class\n","class CalligraphySequence(tf.keras.utils.Sequence):\n","\n","    def __init__(self, image_dir, batch_size):\n","        ### [C1: Build init and len functions]\n","        # Your code here\n","        self.batch_size = batch_size\n","        self.image_dir = image_dir\n","\n","        self.files = os.listdir(self.image_dir)\n","\n","    def __len__(self):\n","        ### [C1: Build init and len functions]\n","        # Your code here\n","        return math.ceil(len(os.listdir(self.image_dir))/self.batch_size)\n","\n","    def __getitem__(self, idx):\n","        ### [C2: Build getitem function]\n","        # Round all pixel values less than 33% of the max 256 value to 0\n","        # anything above this value gets rounded up to 1 so that all values are either\n","        # 0 or 1\n","        # Your code here\n","        \n","        low = idx * self.batch_size\n","        high = min(low + self.batch_size, len(self.files))\n","        batch = self.files[low:high]\n","\n","        imgs = [Image.open(os.path.join(self.image_dir,file)) for file in batch]\n","        for i,img in enumerate(imgs):\n","            grey_pic = img.convert('L')\n","            resize_pic = grey_pic.resize((32,32))\n","            imgs[i] = np.asarray(resize_pic)\n","\n","        layer = tf.keras.layers.Normalization()\n","        layer.adapt(np.array(imgs))\n","        normalized_data = layer(imgs)\n","\n","        binarized = tf.where(normalized_data < 0.33, 0, 1)\n","\n","        return np.array(binarized)[:,:,:,np.newaxis],np.array(binarized)[:,:,:,np.newaxis]\n","\n","# final shape should be 1313 (32, 32, 32, 1) (32, 32, 32, 1)\n","train_ds = CalligraphySequence(data_dir, batch_size)\n","print(len(train_ds), train_ds[0][0].shape, train_ds[0][1].shape)"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["8400\n"]}],"source":["# split into validate dataset\n","files = os.listdir(data_dir)\n","\n","num_files_to_copy = int(len(files) * 0.2)\n","print(num_files_to_copy)\n","for i in range(num_files_to_copy):\n","    os.system(f'cp ./train/{files[i]} ./split_validate/{files[i]}')\n","files = files[::-1]\n","for i in range(len(files) - num_files_to_copy):\n","    os.system(f'cp ./train/{files[i]} ./split_train/{files[i]}')\n"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["1313\n","1050\n","263\n"]}],"source":["validate_ds = CalligraphySequence('./split_validate', batch_size)\n","splitted_train_ds = CalligraphySequence('./split_train', batch_size)\n","print(len(train_ds))\n","print(len(splitted_train_ds))\n","print(len(validate_ds))"]},{"cell_type":"markdown","metadata":{"id":"olVekI5LcdcL"},"source":["## Create layers for the requisite Layers for the model\n"]},{"cell_type":"markdown","metadata":{"id":"VK-wOq1WkaAv"},"source":["### Given function for conv2d / down_shift / right_shift / concat_elu\n","1. conv2d: 2d convolution layer using layers.Conv2D\n","\n","2. down_shift: shift feature down in height dimension (by padding zero to the top and drop the bottom)\n","\n","3. right_shift: shift feature right in width dimension\n","\n","4. concat_elu: a nonlinearity layer (http://arxiv.org/abs/1603.05201)\n","\n","The down_shift and right_shift functions are used to avoid information leaks in a causal network.\n"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1680352340589,"user":{"displayName":"Wing Piu HS","userId":"04946939139592579559"},"user_tz":-480},"id":"EDUbrK8XkZr6"},"outputs":[],"source":["class Conv2d(layers.Layer):\n","    def __init__(self, num_filters, filter_size=[3, 3], stride=[1, 1], pad='SAME', nonlinearity=None, **kwargs):\n","        super().__init__()\n","        self.conv = layers.Conv2D(num_filters, filter_size, padding = pad, strides = stride, activation = nonlinearity, \n","                         kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05))\n","\n","    def call(self, x):\n","        return self.conv(x)\n","\n","def down_move(x, step=1):\n","    input_shape = tf.shape(x)\n","    return tf.concat([tf.zeros((input_shape[0], step, input_shape[2], input_shape[3])), x[:, :input_shape[1] - step, :, :]], 1)\n","\n","def right_move(x, step=1):\n","    input_shape = tf.shape(x)\n","    return tf.concat([tf.zeros((input_shape[0], input_shape[1], step, input_shape[3])), x[:, :, :input_shape[2] - step, :]], 2)\n","\n","def concat_elu(x):\n","    \"\"\" like concatenated ReLU (http://arxiv.org/abs/1603.05201), but then with ELU \"\"\"\n","    axis = len(x.get_shape()) - 1\n","    out = tf.nn.elu(tf.concat([x, -x], axis))\n","    return out"]},{"cell_type":"markdown","metadata":{"id":"QgKeYdBploK-"},"source":["### Gated Residual Block\n","The GatedResnet class applies gated residual connections to input tensors for feature extraction.\n","\n","Please follow Section 4.2.3 to implement coding question.\n"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1680352340590,"user":{"displayName":"Wing Piu HS","userId":"04946939139592579559"},"user_tz":-480},"id":"fy9VWOO0mDHs"},"outputs":[],"source":["class DownMovedConv2d(layers.Layer):\n","    def __init__(self, num_filters, filter_size=[2, 3], stride=[1, 1], pad='VALID', nonlinearity=None, **kwargs):\n","        super().__init__()\n","        ### [C4: Build DownMovedConv2d.]\n","        # Your code here\n","        self.filter_size = filter_size\n","\n","        self.conv = tf.keras.layers.Conv2D(filters=num_filters, kernel_size= self.filter_size, padding = pad, strides = stride, activation = nonlinearity, kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05), data_format='channels_last')\n","        \n","\n","    def call(self, x):\n","        ### [C4: Build DownMovedConv2d.]\n","        # Your code here\n","        paddings = tf.constant([[0,0],[self.filter_size[0]-1,0],[(self.filter_size[1]-1)//2,(self.filter_size[1]-1)//2],[0,0]])\n","        padded = tf.pad(x, paddings=paddings)\n","        return self.conv(padded)\n","\n","\n","class DownRightMovedConv2d(layers.Layer):\n","    def __init__(self, num_filters, filter_size=[2, 2], stride=[1, 1], pad='VALID', nonlinearity=None, **kwargs):\n","        super().__init__()\n","        ### [C3: Build DownRightMovedConv2d.]\n","        # Your code here\n","\n","        self.filter_size = filter_size\n","        self.conv = tf.keras.layers.Conv2D(filters=num_filters, kernel_size=self.filter_size, padding = pad, strides = stride, activation = nonlinearity, kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05), data_format='channels_last')\n","\n","    def call(self, x):\n","        ### [C3: Build DownRightMovedConv2d.]\n","        # Your code here\n","        paddings = tf.constant([[0,0],[self.filter_size[0]-1,0],[self.filter_size[1]-1,0],[0,0]])\n","        padded = tf.pad(x,paddings=paddings)\n","        return self.conv(padded)\n","\n","\n","class TensorDense(layers.Layer):\n","    def __init__(self, num_units, nonlinearity=None, **kwargs):\n","        super().__init__()\n","        ### [C5: Build TensorDense.]\n","        # Your code here\n","        self.num_units = num_units\n","        self.dense = tf.keras.layers.Dense(self.num_units,activation=nonlinearity, kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05))\n","\n","    def call(self, x):\n","        ### [C5: Build TensorDense.]\n","        # Your code here\n","        shape = tf.shape(x)\n","        densed = self.dense(tf.reshape(x, [shape[0]*shape[1]*shape[2],shape[3]]))\n","        return tf.reshape(densed, [shape[0], shape[1], shape[2], self.num_units])\n","\n","\n","class GatedResnet(layers.Layer):\n","    def __init__(self, num_filters, nonlinearity=concat_elu, **kwargs):\n","        super().__init__()\n","        ### [C6: Build GatedResnet.]\n","        # Your code here\n","        self.nnLayer_1 = DownRightMovedConv2d(num_filters)\n","        self.nnLayer_2 = DownRightMovedConv2d(num_filters*2)\n","        self.nonlinearity = nonlinearity\n","\n","    def call(self, x):\n","        ### [C6: Build GatedResnet.]\n","        # Your code here\n","\n","        #1.\n","        y = tf.keras.layers.Activation(self.nonlinearity)(x)\n","        y = self.nnLayer_1(y)\n","\n","        y = tf.keras.layers.Activation(self.nonlinearity)(y)\n","        y = self.nnLayer_2(y)\n","\n","        # 2.feature gated stategy\n","        f,g = tf.split(y, 2, 3)\n","\n","        gated_output = tf.math.multiply(f, tf.math.sigmoid(g))\n","\n","        #3. residual connection\n","        return tf.math.add(x, gated_output)"]},{"cell_type":"markdown","metadata":{"id":"4UFo2cKKoKtv"},"source":["### Main AutoregressiveModel"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1680352340590,"user":{"displayName":"Wing Piu HS","userId":"04946939139592579559"},"user_tz":-480},"id":"s8RHOpB_mHQP"},"outputs":[],"source":["class AutoregressiveModel(layers.Layer):\n","    def __init__(self, n_resnet=5, n_filters=256, n_block=12, n_output=10, **kwargs):\n","        super().__init__()\n","        self.n_resnet = n_resnet\n","        self.n_filters = n_filters\n","        self.n_block = n_block\n","        self.n_output = n_output\n","        # init all network layers\n","        self.down_moved_conv2d = DownMovedConv2d(num_filters=self.n_filters, filter_size=[1, 3])\n","        self.down_right_moved_conv2d = DownRightMovedConv2d(num_filters=self.n_filters, filter_size=[2, 1])\n","\n","        ### [C7: Build AutoregressiveModel.]\n","        # Your code here\n","        self.out_dense = TensorDense(num_units=self.n_output)\n","        self.ul_list_gated_resnet = []\n","        self.ul_list_dense_layer = []\n","        for m in range(self.n_block):\n","            for n in range(self.n_resnet):\n","                self.ul_list_gated_resnet.append(GatedResnet(num_filters=n_filters))\n","            self.ul_list_dense_layer.append(TensorDense(num_units=n_filters))\n","        \n","\n","\n","    def call(self, inputs):\n","        input_shape = tf.shape(inputs)\n","        x = down_move(self.down_moved_conv2d(inputs)) + right_move(self.down_right_moved_conv2d(inputs))\n","        ### [C7: Build AutoregressiveModel.]\n","        # Your code here\n","        # Network Block \n","        for m in range(self.n_block):\n","            for n in range(self.n_resnet):\n","                x = self.ul_list_gated_resnet[m*self.n_resnet+n](x)\n","            x = self.ul_list_dense_layer[m](x)\n","        # 5.\n","        x = tf.keras.layers.Activation(tf.nn.elu)(x)\n","        x_out = self.out_dense(x)\n","\n","\n","        return x_out\n"]},{"cell_type":"markdown","metadata":{"id":"kEF1CtrPcdcO"},"source":["## Build the model based on the original paper\n"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_1 (InputLayer)        [(None, 32, 32, 1)]       0         \n","                                                                 \n"," autoregressive_model (Autor  (None, 32, 32, 10)       3571914   \n"," egressiveModel)                                                 \n","                                                                 \n"," conv2d_74 (Conv2D)          (None, 32, 32, 1)         11        \n","                                                                 \n","=================================================================\n","Total params: 3,571,925\n","Trainable params: 3,571,925\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["## Build the model based on the original paper\n","inputs = keras.Input(shape=input_shape, dtype=tf.float32)\n","x = AutoregressiveModel(n_resnet=6, n_filters=64, n_block=6, n_output=10)(inputs)\n","out = keras.layers.Conv2D(\n","    filters=1, kernel_size=1, strides=1, activation=\"sigmoid\", padding=\"valid\"\n",")(x)\n","\n","pixel_cnn = keras.Model(inputs, out)\n","### [C11: Model training and log reporting]\n","# you can use keras.optimizers.Adam here to define \"adam\"\n","# compile your model and make a summary on its architecture\n","# Your code here\n","\n","\n","pixel_cnn.summary()\n","pixel_cnn.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = 0.0001,\n","beta_1=0.95, beta_2=0.9995, epsilon=1e-6, use_ema=True, ema_momentum=0.9995),\n","              loss=tf.keras.losses.binary_crossentropy,\n","              metrics=['accuracy'])"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2104,"status":"ok","timestamp":1680352342684,"user":{"displayName":"Wing Piu HS","userId":"04946939139592579559"},"user_tz":-480},"id":"mZzvQW8jcdcP","outputId":"3e01f4ea-03bc-4855-afc5-13c32f6a3d06"},"outputs":[{"name":"stderr","output_type":"stream","text":["2023-04-03 16:40:26.662385: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n","\t [[{{node Placeholder/_0}}]]\n"]},{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:5 out of the last 5 calls to <function PreprocessingLayer.make_adapt_function.<locals>.adapt_step at 0x7f2ce0caf4c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"]},{"name":"stderr","output_type":"stream","text":["2023-04-03 16:40:29.368906: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8600\n"]},{"name":"stdout","output_type":"stream","text":[" 2/33 [>.............................] - ETA: 3s - loss: 0.6917 - accuracy: 0.6226  WARNING:tensorflow:6 out of the last 6 calls to <function PreprocessingLayer.make_adapt_function.<locals>.adapt_step at 0x7f2ce0caf280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","33/33 [==============================] - 10s 193ms/step - loss: 0.6918 - accuracy: 0.6191\n"]},{"data":{"text/plain":["[0.6917984485626221, 0.619050145149231]"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["#evaluate with no weight\n","\n","test_ds = CalligraphySequence('./test', batch_size)\n","\n","pixel_cnn.evaluate(x=test_ds)\n"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1680352342684,"user":{"displayName":"Wing Piu HS","userId":"04946939139592579559"},"user_tz":-480},"id":"Eg026cWCwO0t","outputId":"33c6a4a4-9847-4449-f8bb-f1e51554a584"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_1 (InputLayer)        [(None, 32, 32, 1)]       0         \n","                                                                 \n"," autoregressive_model (Autor  (None, 32, 32, 10)       3571914   \n"," egressiveModel)                                                 \n","                                                                 \n"," conv2d_74 (Conv2D)          (None, 32, 32, 1)         11        \n","                                                                 \n","=================================================================\n","Total params: 3,571,925\n","Trainable params: 3,571,925\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n"]}],"source":["### [C8: Load the pretrained weights]\n","# Your code here\n","pixel_cnn.load_weights('pixel_cnn_e5.h5')\n","# model = tf.keras.models.Model.load_weights(pixel_cnn,)\n","print(pixel_cnn.summary())"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3ruvOoJtQyDX","outputId":"e286a941-5860-4111-cbbb-668b30349f71"},"outputs":[{"name":"stderr","output_type":"stream","text":["2023-04-03 16:40:36.793706: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n","\t [[{{node Placeholder/_0}}]]\n"]},{"name":"stdout","output_type":"stream","text":["33/33 [==============================] - 6s 193ms/step - loss: 0.3679 - accuracy: 0.8658\n"]}],"source":["#evalutate with loaded weights\n","history = pixel_cnn.evaluate(x=test_ds)\n"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"OOocGHbkPlsl"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n"]},{"name":"stderr","output_type":"stream","text":["2023-04-03 16:40:43.463567: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n","\t [[{{node Placeholder/_0}}]]\n","2023-04-03 16:40:55.859193: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x7f2b3bd92570 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n","2023-04-03 16:40:55.859230: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): NVIDIA GeForce RTX 2080, Compute Capability 7.5\n","2023-04-03 16:40:55.862904: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n","2023-04-03 16:40:55.960226: I ./tensorflow/compiler/jit/device_compiler.h:180] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"]},{"name":"stdout","output_type":"stream","text":["1050/1050 [==============================] - ETA: 0s - loss: 0.2146 - accuracy: 0.9113"]},{"name":"stderr","output_type":"stream","text":["2023-04-03 16:46:48.135528: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n","\t [[{{node Placeholder/_0}}]]\n"]},{"name":"stdout","output_type":"stream","text":["1050/1050 [==============================] - 415s 372ms/step - loss: 0.2146 - accuracy: 0.9113 - val_loss: 0.2061 - val_accuracy: 0.9140\n","Epoch 2/10\n","1050/1050 [==============================] - 386s 367ms/step - loss: 0.2045 - accuracy: 0.9146 - val_loss: 0.2034 - val_accuracy: 0.9152\n","Epoch 3/10\n","1050/1050 [==============================] - 368s 351ms/step - loss: 0.2021 - accuracy: 0.9157 - val_loss: 0.2017 - val_accuracy: 0.9159\n","Epoch 4/10\n","1050/1050 [==============================] - 366s 349ms/step - loss: 0.2004 - accuracy: 0.9163 - val_loss: 0.2005 - val_accuracy: 0.9163\n","Epoch 5/10\n","1050/1050 [==============================] - 364s 347ms/step - loss: 0.1991 - accuracy: 0.9169 - val_loss: 0.1998 - val_accuracy: 0.9165\n","Epoch 6/10\n","1050/1050 [==============================] - 386s 368ms/step - loss: 0.1981 - accuracy: 0.9173 - val_loss: 0.2000 - val_accuracy: 0.9166\n","Epoch 7/10\n","1050/1050 [==============================] - 388s 369ms/step - loss: 0.1974 - accuracy: 0.9177 - val_loss: 0.1990 - val_accuracy: 0.9170\n","Epoch 8/10\n","1050/1050 [==============================] - 388s 369ms/step - loss: 0.1966 - accuracy: 0.9180 - val_loss: 0.1981 - val_accuracy: 0.9174\n","Epoch 9/10\n","1050/1050 [==============================] - 385s 366ms/step - loss: 0.1957 - accuracy: 0.9183 - val_loss: 0.1981 - val_accuracy: 0.9172\n","Epoch 10/10\n","1050/1050 [==============================] - 386s 368ms/step - loss: 0.1949 - accuracy: 0.9187 - val_loss: 0.1972 - val_accuracy: 0.9178\n"]},{"data":{"text/plain":["<keras.callbacks.History at 0x7f2cd42fc0d0>"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["### [C11: Model training and log reporting]\n","# you can use model.fit here\n","# Your code here\n","pixel_cnn.fit(x=splitted_train_ds, epochs=10, validation_data=validate_ds)"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"LH4M5H4KOh31"},"outputs":[],"source":["# save weights \n","pixel_cnn.save_weights('pixel_cnn_e15.h5')"]},{"cell_type":"markdown","metadata":{"id":"aTlv-hdhcdcQ"},"source":["## Demonstration\n","\n","The AutoregressiveModel cannot generate the full image at once. Instead, it must generate each pixel in\n","order, append the last generated pixel to the current image, and feed the image back into the\n","model to repeat the process."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TBULc1N8cdcS"},"outputs":[{"name":"stdout","output_type":"stream","text":["(10, 32, 32, 1)\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 32/32 [01:33<00:00,  2.92s/it]\n","100%|██████████| 32/32 [01:31<00:00,  2.86s/it]\n","100%|██████████| 32/32 [01:32<00:00,  2.88s/it]\n","100%|██████████| 32/32 [01:32<00:00,  2.90s/it]\n","100%|██████████| 32/32 [01:33<00:00,  2.91s/it]\n","100%|██████████| 32/32 [01:31<00:00,  2.87s/it]\n","100%|██████████| 32/32 [01:33<00:00,  2.91s/it]\n","100%|██████████| 32/32 [01:32<00:00,  2.89s/it]\n","100%|██████████| 32/32 [01:33<00:00,  2.92s/it]\n","100%|██████████| 32/32 [01:33<00:00,  2.91s/it]\n"]}],"source":["from IPython.display import Image, display\n","\n","# Create an empty array of pixels.\n","batch = 10 # you may want to change this parameter \n","pixels = np.zeros(shape=(batch,) + (pixel_cnn.input_shape)[1:])\n","batch, rows, cols, channels = pixels.shape\n","print(pixels.shape)\n","\n","# Iterate over the pixels because generation has to be done sequentially pixel by pixel.\n","for pic in range(batch):\n","    for row in tqdm(range(rows)):\n","        for col in range(cols):\n","            for channel in range(channels):\n","                ### [C9: Qualitative Evaluation]\n","                # Your code here\n","                # 1. Feed the whole array and retrieving the pixel value probabilities for the next\n","                # pixel. You can use model.predict function to get predict value for each pixel.\n","                result = pixel_cnn.predict(pixels, verbose=0)\n","                # 2. Use the probabilities to pick pixel values and append the values to the image\n","                # frame. you can use tf.math.ceil to achieve the 0.5 threshold.\n","                random = tf.random.uniform([],maxval=1)\n","                # print(random)\n","                # print(result[0][row][col][channel])\n","                pixels[pic][row][col][channel] = tf.math.ceil(result[pic][row][col][channel] + tf.random.uniform([],maxval=1) - 1)\n","                # print(pixels[0][row][col][channel])\n","\n","\n","def deprocess_image(x):\n","    # Stack the single channeled black and white image to RGB values.\n","    x = np.stack((x, x, x), 2)\n","    # Undo preprocessing\n","    x *= 255.0\n","    # Convert to uint8 and clip to the valid range [0, 255]\n","    x = np.clip(x, 0, 255).astype(\"uint8\")\n","    return x\n","\n","\n","# Iterate over the generated images and plot them with matplotlib.\n","for i, pic in enumerate(pixels):\n","    keras.preprocessing.image.save_img(\n","        \"generated_image_{}.png\".format(i), deprocess_image(np.squeeze(pic, -1))\n","    )\n","\n","# display(Image(\"generated_image_0.png\"))\n","# display(Image(\"generated_image_1.png\"))\n","# display(Image(\"generated_image_2.png\"))\n","# display(Image(\"generated_image_3.png\"))"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAIAAAD8GO2jAAAAyUlEQVR4nO1WywrEMAjMLPv/v+zeghjfERZK51CKSWcijrFYDES0JgBgv39GGB28Av8X+KpRboPF3AWg6jQlA8G+I/x5JTDVDaaAesYdVOUB7A3i81qRLXZnT0EgZFdTlwJWDUN2q3IFS5wUlmvHLruM3wIBTlHtAEWAiMShBGlDY8CmYwK9DtcvO5U9Y8oTzQycYoilbAaZ8qY6uQ3rBEEGfNQ02D0BUcazP5LTLVsDX96BPtFuhlp/HiRxNdFCKFe6v9zDs35+fxh2XVMuN9OJAAAAAElFTkSuQmCC","text/plain":["<IPython.core.display.Image object>"]},"metadata":{},"output_type":"display_data"},{"data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAIAAAD8GO2jAAAA60lEQVR4nO1WyxLEIAgznf3/X2YPTllKeUmdnjaHTlsFgkEU4wQRjU0AwO/HLqce/gFSfNqWrGRcHQsZAJDlUYSbQUqQiADIUTP8YQ6bU6cvOVTZOpcM7n5jF3N+POcXgOcV1fOs3ACe5R1MfMoQB3OrqFIwMlePkBGgvjLy6cGuojpSNpcMVIylnYUT6n+/VQS05KfWQFVFugJpTRsis019iQIeG9r1tm7a8D6UyOm+b8DoRW2stQq2kZo3qCQBnq9VvtHUOePR96jsuVUEieYaBJ8V5BnI46XIQyLSYImvd71YuyIU8ert+guC7IRTH2wtHwAAAABJRU5ErkJggg==","text/plain":["<IPython.core.display.Image object>"]},"metadata":{},"output_type":"display_data"},{"data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAIAAAD8GO2jAAAA6klEQVR4nK1WQRIDIQhbO/3/l+nBqeNGCBHNqa2QQFCm7ZlgZs8NtNbG588VRoKvEjRXNEPp+KiDSPiagAJJ4GT40gwIUpcqFpmZ3pMkMJfZqfsvikxu0crOj8C0RAAoCg9CnQGhgCP46ncAlc6+u+zERglRjv1BcnOL0or4U2Bn0cXg8RCc34qRQG4RAQ45uumuUW4dTGCLXQlAAZJcMKfDv0VKaXUBkV3cqS+Lth6kaNrGsqvhZdHomuyGI4GVRRkD36bSLoKdwVXB1eLfFnjDZFTO3l+riKh5JBOIMrfMSQTKuDMDAqjyB8L3rhQmh6dxAAAAAElFTkSuQmCC","text/plain":["<IPython.core.display.Image object>"]},"metadata":{},"output_type":"display_data"},{"data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAIAAAD8GO2jAAAA1klEQVR4nMWVSQ6EMAwEacT/vxwOSJbHeI9H9BFMV7wFHExrrWNCAMjwHHF09B2A0vwXgDNKMBF8JaP5Z6VZiHuwOVoBYH9wsyVqqzym1ZyCDHasH+kZvCvTboYCUOvOHwLI9+aLKeKm4uANZOqqCB9a5wsAInpyinxSCSYBogL8suP/qaS7ArBIpIFNVq3Jt5rHTwaDC6wDyNTyfa9FBxCKlyu8NoKa8O0Vm2y1SvxcT/E6l4MZDKC2yb6pxWgCVDtnIsoAn00Y2UURtw/zmjyirSY3dAN7RYE2Atv2bQAAAABJRU5ErkJggg==","text/plain":["<IPython.core.display.Image object>"]},"metadata":{},"output_type":"display_data"},{"data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAIAAAD8GO2jAAAA70lEQVR4nN1WSQ7DMAg0Vf//ZXqwYiGWYamlSp1TomCGZQyhJcDM6waI6Dy/rngE+DMCWTsXRKRs6EGJIPVejCMkYObWYZceERxTGex+ludban5HUaiIttNBlSYqirzbXJfKwEXUGJkTKNokA2YGHtWnpJTSGtQdyC/JoNJVLKq8B2kDMMIM6lWWTLY9PsE2qlyo9PK3B0MUfiQHPYu63lem2t64HuB3Gw0n/i3Brd8LnwB4x9vx2MhXfZNdteViH+xkcMbufcCtCSJrmVlxxIZQ+mGBlNu+JsPOHgbu3CCczT6AHEftnVzB5Z3cIvsA0RqiNn6kqPsAAAAASUVORK5CYII=","text/plain":["<IPython.core.display.Image object>"]},"metadata":{},"output_type":"display_data"},{"data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAIAAAD8GO2jAAAAzElEQVR4nO2UwRKAIAhEo+n/f5kOjg4JwmLMdGlPpbkPIaBDiJmPChHReD5LHB39gE0AEclC1QMKdZmrzDzdYLzqLf/nXt5gdSybOvsG2q7xpDvYlQHAVKrhA8D74fGoQctAKsvUBQFSDLBXdmqgfZ1MeoAwQKRCjxSZB7hLf7aToumw9gqjnqjLYYdb+LIBLUwdrBxEurflegDItoKzCw07WQ+zwolG0+7g7mBARTZP4gwptJPNgju+OcBGxobiFGUnNvSbFuoHfA+4AQABbFjyaZx6AAAAAElFTkSuQmCC","text/plain":["<IPython.core.display.Image object>"]},"metadata":{},"output_type":"display_data"},{"data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAIAAAD8GO2jAAAAwUlEQVR4nO2WwQ7DMAhD8dT//2V2iBRlJAVCPKmH+hhVfpgGWsggVRWGAHTDD8XR0QuY1V/AvwBGFQAAUyYTkLcuAnbxdcCyUfOoniYIO7YNCNeJeaCSYGtlnbYohD1y0PICcLGMRERVmXOwxPDnwBfni+bPGjPB3B8AxXWdxxwlaF7dsYENvnJNm+No5MxzPcGd6XiuqkGCZJmOvASmm1uf4hSAop+KTBMK9c7yElD+hYvb1Nz9u8P4Fi1Nw8MuAF8aDVFJrEgdsgAAAABJRU5ErkJggg==","text/plain":["<IPython.core.display.Image object>"]},"metadata":{},"output_type":"display_data"},{"data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAIAAAD8GO2jAAABAElEQVR4nOVWQRIDIQhbOvv/L9MDo6UBIu7YU3NyWEkQEFeuAVW9zkFEbHH/gtTH+jrODuszAp4R0EpR6k9qpqrTpXUC4IrUYOmmSERi7JOLpCUvcuXg7XZ2Qh3xEeAHV1WfWQLgKYvMC2tfRcQsRDivQSddKaJeIuAvpAGiJnTejgI86mXPzCLBzsVF4+zp8GHwCenDn6C1NUpWzsS+G2XueUxgK3zY/2SabgX4dQ+WU3OJeKzFNE2VZCCyRxIUmD3Hw4yWVo/OTXGxTTewftFgaF/FQ/RQwIiiRh+sTWF4xbxV7cvGtQ8Q/HnbVOskRT7SZkLINnw9TmHvv+i/8QawYfja4RMHQQAAAABJRU5ErkJggg==","text/plain":["<IPython.core.display.Image object>"]},"metadata":{},"output_type":"display_data"},{"data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAIAAAD8GO2jAAAA/klEQVR4nLVWQRIDIQiTTv//ZXpwyjoIIVjLqVORYEJ0ZXxDVcelEBH7/Qr/vRgPwMUTrPHOFuxAPwK/yowudS7/sgZ7kViDmQfIERGyoYCisrrLZAFWVY+F3TeiMQUN8h3UU5QhTQxZogdQ8psluMOlRgv3MEVTH5TV9+6YODcaCfZQZKIxVfhjeYos27kBDBJIGKHI2BCqyt8To3WbgqJgiTLaWkhEMtJDGBbA0c2wjwAc6bMcwOgBzOquLxMWuDqE9ADMeLT8nGqQsYyvo33V+6DszvRwOmX59cWAX9B91YvHdLFu7r7VDaOF01UGC3D8oUe9aAd1Lf7+df0BSNeZYrV4zx8AAAAASUVORK5CYII=","text/plain":["<IPython.core.display.Image object>"]},"metadata":{},"output_type":"display_data"},{"data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAIAAAD8GO2jAAAA70lEQVR4nLVWSw4FIQjTl7n/lX0LE8JAwapMl35aKIr2pjDGaBXovQvhr4Rxkk5eYZ+oEdCkxoayDKDYhoA4kDDO2K8yMBpaVXhvaxDlEWEh4Om0A+KJH6QEdoOFYC2CBWTieBh2oUjO+7kADNCwn5hpKHQl+ZaV9aLEjWjKHKeJtUX+BmmWOZjkZDPg09fXOGqlQOAS3qXHTMNtvgXxkq8M4E6SLmp24Toz6E8tqbquwe5Dffge8EZl74Gf3gJuheQGRhjW+eQeRAcBhkIJ5O9awp7FCDsXZM8BMqj6QIYCtXhZVv75bV9nUPm7jvAHA7ClH8zDVl8AAAAASUVORK5CYII=","text/plain":["<IPython.core.display.Image object>"]},"metadata":{},"output_type":"display_data"}],"source":["for i in range(10):\n","    display(Image(f'generated_image_{i}.png'))"]},{"cell_type":"markdown","metadata":{"id":"VLCjV00RT78w"},"source":["## Quantitative Evaluation\n","\n"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"data":{"text/plain":["<keras.engine.functional.Functional at 0x7f2d3b999190>"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["pixel_cnn"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"yG8oF4E_UPIL"},"outputs":[{"name":"stderr","output_type":"stream","text":["2023-04-03 18:14:36.016182: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n","\t [[{{node Placeholder/_0}}]]\n"]},{"name":"stdout","output_type":"stream","text":["33/33 [==============================] - 7s 198ms/step - loss: 0.1953 - accuracy: 0.9187\n"]},{"data":{"text/plain":["[0.19528882205486298, 0.9186841249465942]"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["### [C10: Quantitative Evaluation]\n","# Your code here\n","\n","# test_ds = CalligraphySequence('./test', batch_size)\n","pixel_cnn.evaluate(x = test_ds)"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"},"vscode":{"interpreter":{"hash":"5238573367df39f7286bb46f9ff5f08f63a01a80960060ce41e3c79b190280fa"}}},"nbformat":4,"nbformat_minor":0}
