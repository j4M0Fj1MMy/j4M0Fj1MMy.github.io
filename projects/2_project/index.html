<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <p><strong>Convolutional Neural Network with IMU-based Gesture-to-Speech Gloves</strong></p> <ul> <li> <strong>Operating System:</strong> Linux, Android</li> <li> <strong>Development Environment:</strong> Tensorflow, Numpy</li> <li> <strong>Language:</strong> Python, shell script</li> <li> <strong>Database:</strong> Flat folder containing csv data files</li> </ul> <p>This project, which was my final year project in college, focused on utilizing a convolutional neural network in conjunction with IMU-based gesture-to-speech gloves for sign language classification. The gloves consisted of five sensors and a Raspberry Pi. We utilized Python, along with the TensorFlow and NumPy libraries, for coding on the Linux server and gloves. Additionally, I wrote shell scripts for hardware control, such as managing Bluetooth and sensors. We also developed a control panel app using Flutter. Throughout the project, we experimented with various machine learning algorithms, including LSTM, RNN, and KNN, but ultimately chose a simple 1-D convolutional network due to its superior speed.</p> <h3 id="introduction"><strong>Introduction</strong></h3> <p>Communication plays an important role in society, however, communicating with the hearing-impaired community could be hard because people seldom have exposure to sign language. This project will work on classifying a sequence of gestures from IMU devices, with a proposed extensible system that could be managed with an application.</p> <p>This project is separated into 3 parts, namely the data fusing pipeline, the classifying model, and the application. The data fusing pipeline is responsible for denoising and sending the data to the application. Then, the classifying model will locate meaningful gestures and turn them into human language. The application hooks everything into a user interface, allowing the user to control the system.</p> <p>This page will share the most important and perhaps the funniest part in the project. But if you are interested in details, feel free to send an email for further discussion !</p> <h3 id="multiplexing-an-i2c-bus"><strong>Multiplexing an I2C bus</strong></h3> <p>As there is only one I2C bus in raspberry pi zero, we did multiplexing ourselves to read from 5 sensors consecutively. We connected all 5 sensors in parallel and connected the circuit to the I2C bus. In each loop, we output a HIGH voltage with a GPIO pin from the RSP to the AD0 pin in MPU6050. This switched the address of the AD0 pin from 0x69 to 0x68, in other words, ‘woke’ the sensor ‘up’. The data of that particular chip was output to the I2C channel to be further processed. In our implementation, the sampling rate of the data is approximately 40Hz which is high enough to display details of each gesture.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/gloves/multiplex-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/gloves/multiplex-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/gloves/multiplex-1400.webp"></source> <img src="/assets/img/gloves/multiplex.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> showcasing the multiplexing connection </div> <h3 id="segmentation-of-a-sequence-of-gestures"><strong>Segmentation of a sequence of gestures</strong></h3> <p>To break a sequence of gestures into separate gestures for our model to handle one by one, we applied a sliding window algorithm. Inside each window, the variance of each column of data is calculated. Then we produced a score based on the variance of the gyroscope columns and the variance of the accelerometer columns. If each of the scores is significantly below its neighbors, we define the window as a breakpoint of gestures. This is because, between each gesture, we observed that there tends to be a temporary pause. If the window contains a pause, the variance will be significantly lower. This idea comes from the skill of NLP. Below is an example of the figure of the sensor values of one finger with a red box indicating a breakpoint.</p> <p>Gallery:</p> <div class="row justify-content-sm-center"> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/gloves/signal-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/gloves/signal-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/gloves/signal-1400.webp"></source> <img src="/assets/img/gloves/signal.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm-4 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/gloves/gloves-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/gloves/gloves-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/gloves/gloves-1400.webp"></source> <img src="/assets/img/gloves/gloves.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> signal of an accelerometer on the gloves, a picture of the prototype </div> <p><strong>Watch a presentation here</strong></p> <figure> <video src="https://cse.hkust.edu.hk/ug/fyp/posters/gallery/2022-2023/107_CSB2_Media.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""></video> </figure> </body></html>